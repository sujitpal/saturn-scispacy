{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Paragraphs from JSON Files\n",
    "\n",
    "This notebook reads the CORD-19 dataset for 2020-08-29 (downloaded from [AllenAI's CORD-19 Historical Releases Page](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html), expanded and manually copied to S3, as follows:\n",
    "\n",
    "    .\n",
    "    ├── pdf_json\n",
    "    ├── pmc_json\n",
    "    └── metadata.csv\n",
    "\n",
    "The `metadata.csv` is a master list of files, some of which are available in the `pdf_json` and `pmc_json` sub-folders. We parse the JSON files and extract paragraphs (and title sentences) and write them out to a Parquet file for downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'medium': 'Medium - 2 cores - 4 GB RAM',\n",
       " 'large': 'Large - 2 cores - 16 GB RAM',\n",
       " 'xlarge': 'XLarge - 4 cores - 32 GB RAM',\n",
       " '2xlarge': '2XLarge - 8 cores - 64 GB RAM',\n",
       " '4xlarge': '4XLarge - 16 cores - 128 GB RAM',\n",
       " '8xlarge': '8XLarge - 32 cores - 256 GB RAM',\n",
       " '12xlarge': '12XLarge - 48 cores - 384 GB RAM',\n",
       " '16xlarge': '16XLarge - 64 cores - 512 GB RAM',\n",
       " 'g4dnxlarge': 'T4-XLarge - 4 cores - 16 GB RAM - 1 GPU',\n",
       " 'g4dn4xlarge': 'T4-4XLarge - 16 cores - 64 GB RAM - 1 GPU',\n",
       " 'g4dn8xlarge': 'T4-8XLarge - 32 cores - 128 GB RAM - 1 GPU',\n",
       " 'p32xlarge': 'V100-2XLarge - 8 cores - 61 GB RAM - 1 GPU',\n",
       " 'p38xlarge': 'V100-8XLarge - 32 cores - 244 GB RAM - 4 GPU',\n",
       " 'p316xlarge': 'V100-16XLarge - 64 cores - 488 GB RAM - 8 GPU'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_saturn.core import describe_sizes\n",
    "\n",
    "describe_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-08 20:52:45] INFO - dask-saturn | Starting cluster. Status: stopped\n",
      "[2020-09-08 20:52:50] INFO - dask-saturn | Starting cluster. Status: starting\n",
      "[2020-09-08 20:53:02] INFO - dask-saturn | Cluster is ready\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1b87bd17d94c349a7588c7d9b3b925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SaturnCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "import time\n",
    "\n",
    "n_workers = 10\n",
    "cluster = SaturnCluster(n_workers=n_workers, scheduler_size='2xlarge', worker_size='4xlarge', nthreads=16)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialized your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    "> **Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for workers, got 0\n",
      "Waiting for workers, got 1\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "while len(client.scheduler_info()['workers']) < n_workers:\n",
    "    print('Waiting for workers, got', len(client.scheduler_info()['workers']))\n",
    "    time.sleep(30)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import Client, progress, get_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"saturn-elsevierinc\"\n",
    "FOLDER_NAME = \"cord19\"\n",
    "\n",
    "DATA_FOLDER = \"/\".join([\"s3:/\", BUCKET_NAME, FOLDER_NAME])\n",
    "\n",
    "METADATA_FILE = \"/\".join([\"s3:/\", BUCKET_NAME, FOLDER_NAME, \"metadata.csv\"])\n",
    "\n",
    "PARAGRAPH_FOLDER = \"/\".join([\"s3:/\", BUCKET_NAME, \"cord19-paras-pq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read metadata file\n",
    "\n",
    "The `metadata.csv` file contains the full list of files. Some of the files don't have full-text associated because of paywall issues, but the `metadata.csv` file contains the title and abstract for them. Files for which full text is provided have the path to the full text referenced in the `pdf_json_files` and `pmc_json_files` columns. \n",
    "\n",
    "Of the files referenced in this dataframe, 140,317 do not have either filepath populated (meaning they don't exist in the dataset), 25,250 have only the `pdf_json_files` column populated, and 4,598 have only the `pmc_json_files` column populated. Our strategy (see cell 7) is to use `pdf_json_files` when available, else use `pmc_json_files`, and discard the record if none are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>pmc_json_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ug7v899j</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
       "      <td>document_parses/pmc_json/PMC35282.xml.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02tnwd4m</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
       "      <td>document_parses/pmc_json/PMC59543.xml.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ejv2xln0</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
       "      <td>document_parses/pmc_json/PMC59549.xml.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b73a28n</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>Endothelin-1 (ET-1) is a 21 amino acid peptide...</td>\n",
       "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
       "      <td>document_parses/pmc_json/PMC59574.xml.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9785vg6d</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>Respiratory syncytial virus (RSV) and pneumoni...</td>\n",
       "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
       "      <td>document_parses/pmc_json/PMC59580.xml.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                              title  \\\n",
       "0  ug7v899j  Clinical features of culture-proven Mycoplasma...   \n",
       "1  02tnwd4m  Nitric oxide: a pro-inflammatory mediator in l...   \n",
       "2  ejv2xln0    Surfactant protein-D and pulmonary host defense   \n",
       "3  2b73a28n               Role of endothelin-1 in lung disease   \n",
       "4  9785vg6d  Gene expression in epithelial cells in respons...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  OBJECTIVE: This retrospective chart review des...   \n",
       "1  Inflammatory diseases of the respiratory tract...   \n",
       "2  Surfactant protein-D (SP-D) participates in th...   \n",
       "3  Endothelin-1 (ET-1) is a 21 amino acid peptide...   \n",
       "4  Respiratory syncytial virus (RSV) and pneumoni...   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/d1aafb70c066a2068b027...   \n",
       "1  document_parses/pdf_json/6b0567729c2143a66d737...   \n",
       "2  document_parses/pdf_json/06ced00a5fc04215949aa...   \n",
       "3  document_parses/pdf_json/348055649b6b8cf2b9a37...   \n",
       "4  document_parses/pdf_json/5f48792a5fa08bed9f560...   \n",
       "\n",
       "                               pmc_json_files  \n",
       "0  document_parses/pmc_json/PMC35282.xml.json  \n",
       "1  document_parses/pmc_json/PMC59543.xml.json  \n",
       "2  document_parses/pmc_json/PMC59549.xml.json  \n",
       "3  document_parses/pmc_json/PMC59574.xml.json  \n",
       "4  document_parses/pmc_json/PMC59580.xml.json  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = dd.read_csv(METADATA_FILE, dtype=str)\n",
    "metadata_df = metadata_df[[\"cord_uid\", \"title\", \"abstract\", \n",
    "                           \"pdf_json_files\", \"pmc_json_files\"]]\n",
    "\n",
    "# :TODO: comment for real run\n",
    "# metadata_df = metadata_df.sample(frac=0.0005)\n",
    "\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each record, we read the referenced file from the S3 filesystem, parse it into a JSON dictionary, and extract the text blocks we are interested in, namely the title, abstract (multiple paragraphs) and body (multiple paragraphs). We also compute the sequence number for each paragraph. This array of tuples (`pid`, `ptext`) is returned by the `parse_paragraphs` function below.\n",
    "\n",
    "We then explode the `paragraphs` column, and separate out the `pid` and `ptext` columns, then write them out into a set of Parquet files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fully(filepath, s3, bucket_name):\n",
    "    obj = s3.Object(bucket_name, filepath)\n",
    "    s = obj.get()['Body'].read().decode('utf-8')\n",
    "    return s\n",
    "\n",
    "\n",
    "def parse_paragraphs(rows, bucket_name):\n",
    "    worker = get_worker()\n",
    "    try:\n",
    "        s3 = worker.s3\n",
    "    except:\n",
    "        s3 = boto3.resource('s3')\n",
    "        worker.s3 = s3\n",
    "    paragraphs = []\n",
    "    filepath = None\n",
    "    try:\n",
    "        if pd.notnull(rows.pdf_json_files):\n",
    "            filepath = rows.pdf_json_files\n",
    "        elif pd.notnull(rows.pmc_json_files):\n",
    "            filepath = rows.pmc_json_files\n",
    "        else:\n",
    "            pass\n",
    "        if filepath is not None:\n",
    "            abs_filepath = filepath.replace(\"document_parses\", \"cord19\")\n",
    "            fdict = json.loads(read_fully(abs_filepath, s3, bucket_name))\n",
    "            paragraphs.append((\"T\", fdict[\"metadata\"][\"title\"]))\n",
    "            paragraphs.extend([(\"A{:d}\".format(i), x[\"text\"]) \n",
    "                for i, x in enumerate(fdict[\"abstract\"])])\n",
    "            paragraphs.extend([(\"B{:d}\".format(i), x[\"text\"]) \n",
    "                for i, x in enumerate(fdict[\"body_text\"])])\n",
    "        else:\n",
    "            paragraphs.append((\"T\", rows[\"title\"]))\n",
    "            for i, abs_para_text in enumerate(rows[\"abstract\"].split('\\n')):\n",
    "                paragraphs.append((\"A{:d}\".format(i), abs_para_text))\n",
    "    except:\n",
    "        pass\n",
    "    return paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df = metadata_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df = paragraph_df.repartition(npartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_df[\"paragraphs\"] = paragraph_df.apply(\n",
    "    lambda rows: parse_paragraphs(rows, BUCKET_NAME), meta=(\"object\"), axis=1)\n",
    "paragraph_df = paragraph_df.drop(columns=[\"title\", \"abstract\",\n",
    "                                          \"pdf_json_files\", \"pmc_json_files\"])\n",
    "paragraph_df = paragraph_df.explode(\"paragraphs\")\n",
    "paragraph_df = paragraph_df.dropna()\n",
    "paragraph_df[\"pid\"] = paragraph_df.apply(lambda rows: rows.paragraphs[0], \n",
    "                                         meta=(\"str\"), axis=1)\n",
    "paragraph_df[\"ptext\"] = paragraph_df.apply(lambda rows: rows.paragraphs[1], \n",
    "                                           meta=(\"str\"), axis=1)\n",
    "paragraph_df = paragraph_df.drop(columns=[\"paragraphs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "if fs.exists(PARAGRAPH_FOLDER):\n",
    "    fs.rm(PARAGRAPH_FOLDER, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://saturn-elsevierinc/cord19-paras-pq'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAPH_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 ms, sys: 31.1 ms, total: 800 ms\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paragraph_df.to_parquet(PARAGRAPH_FOLDER, engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://saturn-elsevierinc/cord19-paras-pq'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAGRAPH_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls(PARAGRAPH_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.du(PARAGRAPH_FOLDER) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Passed non-file path: saturn-elsevierinc/cord19-paras-pq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2fb9c2beec7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparagraph_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARAGRAPH_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparagraph_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/saturn/lib/python3.7/site-packages/dask/dataframe/io/parquet/core.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, gather_statistics, split_row_groups, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msplit_row_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_row_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/saturn/lib/python3.7/site-packages/dask/dataframe/io/parquet/arrow.py\u001b[0m in \u001b[0;36mread_metadata\u001b[0;34m(cls, fs, paths, categories, index, gather_statistics, filters, split_row_groups, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# correspond to a row group (populated below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         parts, dataset = _determine_dataset_parts(\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         )\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Check if the column-chunk file_path's are set in \"_metadata\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/saturn/lib/python3.7/site-packages/dask/dataframe/io/parquet/arrow.py\u001b[0m in \u001b[0;36m_determine_dataset_parts\u001b[0;34m(fs, paths, gather_statistics, filters, dataset_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# There is only one file to read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/saturn/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size)\u001b[0m\n\u001b[1;32m   1028\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_manifest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m              \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m              \u001b[0mopen_file_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_dataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         )\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/saturn/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m_make_manifest\u001b[0;34m(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 raise IOError('Passed non-file path: {0}'\n\u001b[0;32m-> 1229\u001b[0;31m                               .format(path))\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0mpiece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetDatasetPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_file_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen_file_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Passed non-file path: saturn-elsevierinc/cord19-paras-pq"
     ]
    }
   ],
   "source": [
    "paragraph_df = dd.read_parquet(PARAGRAPH_FOLDER, engine=\"pyarrow\")\n",
    "paragraph_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# do this if youre done using the cluster\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
